---
title: "Module 2: Output Parsers, Chains, Context Window, Chat History, Trimmer, Chatbot, Memory"
layout: post
---

Explore key components that allow LangChain to build sophisticated and dynamic workflows, such as chains that connect multiple language model calls or tools in sequence, parsers that interpret and structure raw outputs into usable formats, and memory that maintains conversational context across interactions to enable stateful agents. By the end, you will understand how these elements work together to create richer, multi-step applications that go beyond simple single-prompt interactions.


*The video below is a recap of what we covered in the previous module and also gives a brief introduction to the concepts of parsers and chains, which we will explore in more depth throughout this module.*

{% include embed.html url="https://www.youtube.com/embed/mrjq3lFz23s" %}

---

# **1. Output Parsers**

Output parsers in LangChain are essential tools that transform raw, unstructured responses from language models into clean, structured formats like strings, lists, dictionaries, or validated objects. This is important because raw text can often be unpredictable, inconsistent, or hard to process directly in code. By using output parsers, you ensure that outputs follow a predictable structure, making them easier to integrate into applications, trigger follow-up actions, or pass to downstream systems.

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/concepts/output_parsers/" target="_blank">
      <img src="{{ '/assets/images/langchain_output_parsers.png' | relative_url }}" alt="Langchain Output Parsers"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        Check the official documentation on output parsers and the different types of output parsers available, so you can understand their purpose and choose the right one for your specific use case.
      </em>
    </p>
  </div>

</div>

LangChain offers a variety of built-in output parsers, such as `StrOutputParser` for plain text, `CommaSeparatedListOutputParser` for lists, and `PydanticOutputParser` for structured data that follows defined schemas. These tools help reduce the risk of errors, improve automation reliability, and enforce data quality standards, especially when chaining multiple steps together or when interacting with external tools and APIs that require well-formed, consistent input.

*Check the video below to better understand what output parsers are and how they help improve the results you get from your models. The video covers key types such as the Structured Output Parser, CommaSeparatedList Output Parser, Pydantic Output Parser, Output Fixing Parser, and Retry Output Parser, with practical examples of when and how to use each.*

{% include embed.html url="https://www.youtube.com/embed/UVn2NroKQCw" %}

---

# **2. Chains**

In LangChain, a Chain is a structured workflow where data moves through a series of components, with each component performing a specific task. Instead of relying on one big model call, chains break the process into smaller, modular steps — for example, formatting a prompt, calling a language model, and parsing the output.

This modular design makes it easier to:

- Connect multiple steps into a meaningful sequence
- Reuse common workflows across different applications
- Hide underlying complexity, so you can focus on the bigger logic

In short, a chain is like a pipeline that transforms inputs into outputs by passing them through well-defined stages.

### LCEL (LangChain Expression Language)

LCEL is the modern approach to building chains in LangChain. It provides a more flexible and composable way to create chains using a declarative syntax for composing **Runnables** into chains using the pipe operator `|`. This approach simplifies the construction of complex workflows by allowing you to define the sequence of operations in a readable and concise manner.

> But wait — before we continue: **what exactly is a Runnable?**

It represents any unit of work that can process inputs and produce outputs — for example, a language model, a prompt template, an output parser, a retriever, or even an entire chain.

What makes Runnables powerful is that they all share a **standardized interface**. No matter what specific task they perform, they provide the same key methods:

`.invoke() `→ processes a single input synchronously and returns the output.

`.batch()` → processes a list of inputs in parallel and returns a list of outputs.

`.stream()` → processes an input and streams the output incrementally.

`.astream_log()` → streams both the final output and selected intermediate steps.

With this foundation in mind, we can now understand why LCEL is so effective: by treating all components as Runnables, LCEL allows you to seamlessly compose them into powerful, multi-step workflows using the simple pipe syntax. Whether you're connecting models, parsers, retrievers, or custom functions, LCEL provides a unified, elegant way to chain these pieces together — all while automatically supporting synchronous, asynchronous, batch, and streaming execution.

*Now let's look at an example that puts into practice the concepts of LCEL, chains, and output parsers, showing how they work together to create a seamless and structured workflow.*

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# Define a prompt template
prompt = ChatPromptTemplate.from_template(
    "Write a 5-line poem about {topic}."
)

# Initialize a model
model = ChatOpenAI(model="gpt-3.5-turbo")

# Use an output parser to get a string
output_parser = StrOutputParser()

# Build the chain with LCEL
chain = prompt | model | output_parser

# Run the chain with .invoke()
result = chain.invoke({"topic": "AI Agents, Education and Seeds"})
print(result)
```
*Output:*
```
AI agents sow seeds of knowledge,
In the fertile fields of education,
Nurturing growth and understanding,
With algorithms and innovation,
Harvesting a future of endless possibilities.
```

*Below are two articles that provide a deeper explanation of chains and output parsers, respectively. I recommend reviewing them to strengthen your understanding of how these components work.*

<div style="display: flex; gap: 20px; justify-content: center; flex-wrap: nowrap;">
  
  <a href="https://medium.com/@shravankoninti/different-chain-types-using-langchain-89cacae6ad1f" target="_blank" style="flex: 1 1 28%; box-sizing: border-box; overflow: hidden;">
    <img src="{{ '/assets/images/langchain_chain_types.png' | relative_url }}" alt="Langchain Chain Types"
         style="width: 100%; height: 93%; object-fit: cover; border-radius: 8px;" />
  </a>

  <a href="https://medium.com/donato-story/unpacking-output-parsers-with-langchain-d84ccaff8d3c" target="_blank" style="flex: 1 1 28%; box-sizing: border-box;">
    <img src="{{ '/assets/images/medium_parsers.png' | relative_url }}" alt="Medium Parsers"
         style="width: 100%; height: auto; object-fit: cover; border-radius: 8px;" />
  </a>

</div>


*Here are the official documentation references for the terms we've discussed. While reviewing them is optional, it's highly recommended since they are the most reliable source of information, offering deeper explanations, practical examples, and helping you develop a stronger, more accurate understanding of these concepts in action.*

<div style="display: flex; gap: 20px; justify-content: center; flex-wrap: wrap;">

  <a href="https://python.langchain.com/docs/concepts/lcel/" target="_blank" style="flex: 1 1 22%; box-sizing: border-box;">
    <img src="{{ '/assets/images/lc_lcel.png' | relative_url }}" alt="Langchain LCEL"
         style="width: 100%; height: auto; border-radius: 8px;" />
  </a>

  <a href="https://python.langchain.com/docs/how_to/sequence/" target="_blank" style="flex: 1 1 22%; box-sizing: border-box;">
    <img src="{{ '/assets/images/lc_chain.png' | relative_url }}" alt="Langchain How To Chain Runnables"
         style="width: 100%; height: auto; border-radius: 8px;" />
  </a>

  <a href="https://python.langchain.com/docs/how_to/parallel/" target="_blank" style="flex: 1 1 22%; box-sizing: border-box;">
    <img src="{{ '/assets/images/lc_parallel.png' | relative_url }}" alt="Langchain Runnables in Parallel"
         style="width: 100%; height: auto; border-radius: 8px;" />
  </a>

  <a href="https://python.langchain.com/docs/concepts/runnables/" target="_blank" style="flex: 1 1 22%; box-sizing: border-box;">
    <img src="{{ '/assets/images/lc_runnable.png' | relative_url }}" alt="Langchain Runnable Interface"
         style="width: 100%; height: auto; border-radius: 8px;" />
  </a>

</div>

---

# **2. Context Window**

Before we dive deeper into topics like memory, message history management, and how chatbots keep track of conversations, **we first need to understand the concept of the context window** in Large Language Models.

The context window is basically the amount of information — measured in tokens — that the model can “see” and process at once. This includes not just the most recent user message, but also past conversation turns, instructions, and the model's own generated responses.

Why is this important? Because LLMs don't have memory like humans. They can only work with what's inside this limited window. If your conversation or task is too long and goes beyond the window size, earlier parts get cut off or lost.

*Please check the video below to better understand the concept of a context window:*

{% include embed.html url="https://www.youtube.com/embed/-QVoIxEpFkM" %}

---

# **3. Chat History**

**Chat History** refers to the chronological record of interactions between a user and a chat model. It maintains the context and state of a conversation by storing a sequence of messages, each tagged with a specific **role** to indicate its source or purpose.

We have already talked about the different **roles** like **user**, **assistant**, **system**, and **tool**, which help define the source and purpose of each message in a conversation. Now, connecting this to the concept of **chat history**, it's important to understand how these roles come together in practice.

Most conversations begin with a **system message** that sets the initial instructions or context for the AI's behavior, or they start directly with a **user message**. From there, the core dialogue unfolds through alternating **user** and **assistant** messages, where the user provides input, and the assistant responds. Additionally, when the assistant needs to use external tools — for example, a weather service to fetch current weather data — it can trigger **tool messages** that return the results of those tool invocations.

Altogether, these role-labeled messages form the structured chat history, allowing the system to maintain context and handle the conversation effectively over time.

Example flow:  
```shell
System: "You are a helpful assistant."  
User: "What's the weather in Paris today?"  
Assistant: *Calls a weather API tool*  
Tool: Returns JSON weather data
Assistant: "It's 22°C and sunny in Paris."  
```

### Managing Chat History

Because the chat history carries all messages across the conversation, including tool interactions, it can quickly grow in size. However, language models have a **context window** — a maximum number of tokens they can process at once. To handle this, we need to carefully manage the chat history, trimming older or less relevant parts when necessary. Importantly, this trimming must preserve the structural integrity of the conversation: for example, the first message (often a **system** or **user** message) should remain, recent important exchanges should be kept, and **tool messages** must always directly follow the assistant’s tool calls to make sense. The last message in the history should come from the **user** or **tool**, ensuring the model has the correct context for generating the next response.

> **Why It Matters:**

**Context Preservation**: Chat history ensures the AI remembers prior interactions, allowing it to handle follow-up questions smoothly.

**Tool Integration**: Properly structured history enables the assistant to use external tools dynamically and weave the results into the conversation.

**Efficiency**: Effective management of chat history avoids errors that can arise from exceeding the context window or from breaking the logical order of the conversation.

> **Agentic Workflows**
>
> *Although we haven’t fully explored what an “agent” is yet, here’s a little preview: when the assistant begins to use external tools — like calling a weather service to fetch live data — it starts behaving in a more agentic way. This means it’s not just passively answering user questions but actively planning and executing steps to complete tasks.
In these cases, the assistant identifies that it needs extra information, makes a tool call to perform the necessary action, receives the tool message with the result, and then processes that result to give a meaningful response back to the user. All of this interaction is carefully recorded in the chat history, ensuring that the conversation remains coherent and context-aware.
> We’ll dive deeper into the concept of agents later, but for now, it’s useful to recognize how these more advanced workflows begin to appear within the chat history itself.*

---

# **3. Trimmer**

In conversational AI applications, such as chatbots, managing the length of the conversation history is crucial due to the limited context window of language models. LangChain addresses this challenge with the `trim_messages` utility, which helps reduce the size of a chat history to fit within a specified token count or message count, ensuring the model receives relevant and coherent context.

For instance, you can configure `trim_messages` to retain only the most recent messages, ensuring that the chat history starts with a `HumanMessage` or a `SystemMessage` followed by a `HumanMessage`, and ends with a `HumanMessage` or a `ToolMessage`. This setup maintains the logical structure expected by most chat models.

Here's a simple example:

```python
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
    trim_messages,
)
from langchain_openai import ChatOpenAI

# Initial full message history
messages = [
    SystemMessage(content="You are a helpful assistant specialized in restaurant recommendations."),               
    HumanMessage(content="Hi, can you recommend a good Italian restaurant nearby?"),      
    AIMessage(content="Sure! Are you looking for something casual or fine dining?"),           
    HumanMessage(content="Something casual, please."),                
    AIMessage(content="Got it. Would you like me to check the top-rated option on Google Maps?"),        
    HumanMessage(content="Yes, please do."),       
    AIMessage(content="Checking now..."),                           
    ToolMessage(content="Top casual Italian restaurant nearby: Luigi’s Trattoria", tool_call_id="restaurant-finder"),  
    AIMessage(content="I’ve found a great option for you: Luigi’s Trattoria. Would you like me to book a table or show the menu?"),     
]

# Trim to fit max 100 tokens
trimmed = trim_messages(
    messages,
    max_tokens=100,
    strategy="last",
    token_counter=ChatOpenAI(model="gpt-4o"),
    start_on="human",
    end_on=("human", "tool"),
    include_system=True,
)

# Output trimmed history
for msg in trimmed:
    print(f"{msg.type}: {msg.content}")
```

> *Output:*
>
> `system`: You are a helpful assistant specialized in restaurant recommendations.
>
> `human` : Something casual, please.
>
> `ai`    : Got it. Would you like me to check the top-rated option on Google Maps?
>
> `human` : Yes, please do.
>
> `ai`    : Checking now...
>
> `tool`  : Top casual Italian restaurant nearby: Luigi’s Trattoria

*Notice how only the most recent and essential messages are kept — earlier parts of the conversation (like the initial interaction) have been trimmed away to fit within the token limit, while the system still preserves the flow and context needed for the assistant to continue the conversation smoothly.*

<div style="display: flex; align-items: flex-start; gap: 20px;">
  
  <div style="flex: 0 0 25%;">
    <a href="https://python.langchain.com/docs/how_to/trim_messages/" target="_blank">
      <img src="{{ '/assets/images/langchain_trim.png' | relative_url }}" alt="Langchain Trim"
           style="width: 100%; height: auto; border-radius: 8px;" />
    </a>
  </div>
  
  <div style="flex: 1;">
    <p>
      <em>
        To delve deeper into how to effectively implement and configure trim_messages in your applications — ensuring you manage conversation history efficiently and maintain smooth, coherent interactions — I highly recommend following the official LangChain tutorial provided below:
      </em>
    </p>
  </div>

</div>

---

# **4. Chatbot**
